Strata Data Pipelines
====================

## Build

```
docker build . -t data-pipelines:latest
```

## Run Anchor Localnet

Clone your anchor repo (if capturing events for anchor). Otherwise, follow similar steps for you Solana setup.

Run

```
anchor localnet
```

Then upload your idl(s) with

```
anchor idl init <program-id> --filepath <idl.json> --provider.cluster localnet
```

## Run Data Pipelines

First, update `ACCOUNTS` and `ANCHOR_IDLS` in docker-compose.yml to the programs you would like to capture, and the anchor programs you would like to parse.

Run

```
docker-compose up
```

In this repo. You can also run a subset, for example only run up to the event transformer:

```
docker-compose up event-transformer
```

## Run Strata

If you're doing local dev for strata, you'll want our leaderboards. 

```
cd strata-compose && docker-compose up
```

## Setup kSQL


# Components

## Kafka S3 Block Uploader

This utility pulls blocks for each contiguous Solana slot and inserts them into S3. It then sends an event pointing to that s3 location to Kafka. We avoid sending the full block to kafka as it may be too large of a message.

## Event Transformer

Reads the events from Kafka S3 Block Uploader, pulls the blocks from S3, and transforms the transaction data into usable JSON events. Each event has common fields like `type`, `blockTime`, `slot`.

This gives us a fat topic of all events occurring on the blockchain

## ksqlDB

Looking at `queries.sql`, you can see all of our [ksqlDB](https://docs.ksqldb.io/en/latest) queries. These queries turn the firehose of `events` into useful tables and streams.

The main usecase right now for these streams is to create leaderboards both on holders of individual accounts, and on total WUM locked

## Leaderboard Redis Inserters

These read from streams generated by ksqlDB and insert them into Redis sorted sets so that we can power a fast graphQL API.

# Deploying

Docker build can be done via

```bash
aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin 554418307194.dkr.ecr.us-east-2.amazonaws.com
docker build --build-arg NPM_TOKEN=$NPM_TOKEN .
```

You should use the `strata-terraform` repo to deploy the full pipeline. We use app.terraform.io to provision and launch terraform objects on AWS.

# Local Development

First, boot up docker compose:

```bash
docker-compose up
```

Then, create the bucket we're going to use in minio by going to localhost:9000. The username and password are both `minioadmin`.

Create a bucket named `solana-blocks`

Next, start up the Process Blocks utility. (This is the Kafka S3 Block Uploader) There should be a vscode task for this.

Next, start up the Event Transform utility. There should be a vscode task for this.

Now, events should be streaming into kafka.

Let's load the ksql:

```bash
docker exec -it ksqldb-cli ksql http://ksqldb-server:8088
```

Open up the sql files in ksql/ and copy them into the command line.

You can use kowl at localhost:8080 to see what's going into the topics.

If you'd like to start the leaderboard, there are vscode launch configs for those as well.

The API is a separate repo, `strata-api`

# Trophies

To test, you can run 

```
jq -rc . tests/resources/trophy.json | kafka-console-producer.sh --topic json.solana.trophies --bootstrap-server localhost:29092
```
