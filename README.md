Wumbo Data Pipeline
====================

## Install Deps

### Npm login
Login with npm, `npm login`

Then,

```bash
npm token create --read-only
```

Copy that value as an env var in your bashrc, NPM_TOKEN

## Install

```
yarn install
```


# Components

## Kafka S3 Block Uploader

This utility pulls blocks for each contiguous Solana slot and inserts them into S3. It then sends an event pointing to that s3 location to Kafka. We avoid sending the full block to kafka as it may be too large of a message.

## Event Transformer

Reads the events from Kafka S3 Block Uploader, pulls the blocks from S3, and transforms the transaction data into usable JSON events. Each event has common fields like `type`, `blockTime`, `slot`.

This gives us a fat topic of all events occurring on the blockchain

## ksqlDB

Looking at `queries.sql`, you can see all of our [ksqlDB](https://docs.ksqldb.io/en/latest) queries. These queries turn the firehose of `events` into useful tables and streams.

The main usecase right now for these streams is to create leaderboards both on holders of individual accounts, and on total WUM locked

## Leaderboard Redis Inserters

These read from streams generated by ksqlDB and insert them into Redis sorted sets so that we can power a fast graphQL API.

# Deploying

Docker build can be done via

```bash
aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin 554418307194.dkr.ecr.us-east-2.amazonaws.com
docker build --build-arg NPM_TOKEN=$NPM_TOKEN .
```

You should use the `wumbo-terraform` repo to deploy the full pipeline. We use app.terraform.io to provision and launch terraform objects on AWS.

# Local Development

First, boot up docker compose:

```bash
docker-compose up
```

Then, create the bucket we're going to use in minio by going to localhost:9000. The username and password are both `minioadmin`.

Create a bucket named `wumbo-solana-blocks`

Next, start up the Process Blocks utility. (This is the Kafka S3 Block Uploader) There should be a vscode task for this.

Next, start up the Event Transform utility. There should be a vscode task for this.

Now, events should be streaming into kafka.

If you're starting fresh, your local kafka wont know anything about the existing dev wumbo instance, as those initialize events are long since past. You should instead create a new one.

Go to `/wumbo/rust/` and run `./bootstrap.sh`. Copy the created values into globals.tsx and copy paste the WUM mint into the queries inserting into `wum_locked_by_account`.

 Let's load the ksql:

```bash
docker exec -it ksqldb-cli ksql http://ksqldb-server:8088
```

Open up queries.sql and copy them into the command line.

You can use kowl at localhost:8080 to see what's going into the topics.

If you'd like to start the leaderboard, there are vscode launch configs for those as well.

The API is a separate repo, `wumbo-api`
